{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":296262172,"sourceType":"kernelVersion"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# BATCHED EXTRACTION: Process subjects in batches\n# Upload to OneDrive between batches to manage space\n# ============================================================\n# \n# CONFIGURATION:\n# - Change BATCH_NUMBER before each run (1, 2, 3, ...)\n# - Each batch processes ~10 subjects (~5,500 volumes)\n# - 62 subjects / 10 = 7 batches total (for 4 classes)\n# - For all 13 classes: ~16 batches\n#\n# BATCH PLAN (4 classes only):\n# Batch 1: sub-01 to sub-10\n# Batch 2: sub-11 to sub-20  \n# Batch 3: sub-21 to sub-30\n# Batch 4: sub-31 to sub-40\n# Batch 5: sub-41 to sub-50\n# Batch 6: sub-51 to sub-60\n# Batch 7: sub-61 to sub-68 (remaining subjects)\n# ============================================================\n\n# ==================== CONFIGURATION ====================\n# CHANGE THIS FOR EACH RUN!\nBATCH_NUMBER = 7  # Change to 1, 2, 3, 4, 5, 6, 7\n\n# Subjects per batch (adjust if needed)\nSUBJECTS_PER_BATCH = 10\n\n# Only extract these 4 classes (saves space!)\nEXTRACT_4_CLASSES_ONLY = True\n# ========================================================\n\n# ==================== CELL 1: Setup ====================\n!pip install -q boto3 nibabel tqdm\n\nimport os\nimport numpy as np\nimport nibabel as nib\nfrom scipy.ndimage import zoom\nfrom pathlib import Path\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport boto3\nfrom botocore import UNSIGNED\nfrom botocore.config import Config\nimport warnings\nimport shutil\nwarnings.filterwarnings('ignore')\n\n# S3 client\ns3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\nBUCKET = 'openneuro.org'\nDATASET = 'ds004044'\n\n# Paths\nRAW_DIR = Path('/kaggle/working/raw_data')\nOUTPUT_DIR = Path(f'/kaggle/working/batch_{BATCH_NUMBER:02d}')\nRAW_DIR.mkdir(exist_ok=True)\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Target shape\nTARGET_SHAPE = (100, 100, 100)\nTR = 2.0\n\n# Trial type mapping\nTRIAL_TYPE_MAP = {\n    0: 'Rest',\n    1: 'Toe movements',\n    2: 'Ankle movements',\n    3: 'Left leg movements',\n    4: 'Right leg movements',\n    5: 'Forearm movements',\n    6: 'Upper arm movements',\n    7: 'Wrist movements',\n    8: 'Finger movements',\n    9: 'Eye movements',\n    10: 'Jaw movements',\n    11: 'Lip movements',\n    12: 'Tongue movements',\n}\n\n# Classes to extract\nif EXTRACT_4_CLASSES_ONLY:\n    CLASSES_TO_EXTRACT = {3, 4, 5, 6}\n    print(\"Extracting 4 classes: Left leg, Right leg, Forearm, Upper arm\")\nelse:\n    CLASSES_TO_EXTRACT = None  # All classes\n    print(\"Extracting ALL 13 classes\")\n\n# Create output directories\nfor code, class_name in TRIAL_TYPE_MAP.items():\n    if CLASSES_TO_EXTRACT is None or code in CLASSES_TO_EXTRACT:\n        (OUTPUT_DIR / class_name).mkdir(exist_ok=True)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"BATCH {BATCH_NUMBER}\")\nprint(f\"{'='*60}\")\n\n# ==================== CELL 2: Get subject list for this batch ====================\n\ndef list_all_subjects():\n    \"\"\"List all subjects with denoised data\"\"\"\n    paginator = s3.get_paginator('list_objects_v2')\n    subjects = set()\n    \n    for page in paginator.paginate(Bucket=BUCKET, Prefix=f'{DATASET}/derivatives/fmriprep/', Delimiter='/'):\n        for prefix in page.get('CommonPrefixes', []):\n            # Extract subject ID from path like 'ds004044/derivatives/fmriprep/sub-01/'\n            parts = prefix['Prefix'].rstrip('/').split('/')\n            if len(parts) >= 4 and parts[3].startswith('sub-'):\n                subjects.add(parts[3])\n    \n    return sorted(list(subjects))\n\nprint(\"Listing all subjects...\")\nall_subjects = list_all_subjects()\nprint(f\"Total subjects in dataset: {len(all_subjects)}\")\n\n# Calculate batch range\nstart_idx = (BATCH_NUMBER - 1) * SUBJECTS_PER_BATCH\nend_idx = min(start_idx + SUBJECTS_PER_BATCH, len(all_subjects))\n\nbatch_subjects = all_subjects[start_idx:end_idx]\n\nprint(f\"\\nBatch {BATCH_NUMBER}: Subjects {start_idx + 1} to {end_idx}\")\nprint(f\"Processing: {batch_subjects}\")\n\nif not batch_subjects:\n    print(\"\\n*** NO SUBJECTS IN THIS BATCH - ALL DONE! ***\")\n    raise SystemExit(\"Batch complete\")\n\n# ==================== CELL 3: Helper Functions ====================\n\ndef list_subject_files(subject_id):\n    \"\"\"List all denoised files for a subject\"\"\"\n    paginator = s3.get_paginator('list_objects_v2')\n    files = []\n    \n    for page in paginator.paginate(Bucket=BUCKET, Prefix=f'{DATASET}/derivatives/fmriprep/{subject_id}/'):\n        for obj in page.get('Contents', []):\n            if 'denoised' in obj['Key'] and obj['Key'].endswith('.nii.gz'):\n                files.append(obj['Key'])\n    \n    return sorted(files)\n\ndef get_events_key(denoised_key):\n    \"\"\"Convert denoised file path to events file path\"\"\"\n    parts = denoised_key.split('/')\n    subject = parts[3]\n    filename = parts[4]\n    run_part = filename.split('_run-')[1].split('_')[0]\n    run_num = int(run_part)\n    events_key = f\"{DATASET}/{subject}/ses-1/func/{subject}_ses-1_task-motor_run-{run_num:02d}_events.tsv\"\n    return events_key\n\ndef download_file(s3_key, local_path):\n    \"\"\"Download a file from S3\"\"\"\n    local_path = Path(local_path)\n    local_path.parent.mkdir(parents=True, exist_ok=True)\n    if not local_path.exists():\n        try:\n            s3.download_file(BUCKET, s3_key, str(local_path))\n            return True\n        except Exception as e:\n            print(f\"    Error: {e}\")\n            return False\n    return True\n\ndef preprocess_volume(volume_3d):\n    \"\"\"Resize and normalize\"\"\"\n    zoom_factors = [t / s for t, s in zip(TARGET_SHAPE, volume_3d.shape)]\n    resized = zoom(volume_3d, zoom_factors, order=1)\n    mean, std = resized.mean(), resized.std()\n    normalized = (resized - mean) / std if std > 0 else resized - mean\n    return normalized.astype(np.float32)\n\ndef extract_volumes(denoised_path, events_path, subject_id, run_id):\n    \"\"\"Extract all volumes from 4D fMRI\"\"\"\n    img = nib.load(denoised_path)\n    data_4d = img.get_fdata()\n    n_volumes = data_4d.shape[3]\n    \n    events = pd.read_csv(events_path, sep='\\t')\n    counts = {}\n    \n    for _, row in events.iterrows():\n        onset = row['onset']\n        duration = row['duration']\n        trial_type = int(row['trial_type'])\n        \n        if CLASSES_TO_EXTRACT is not None and trial_type not in CLASSES_TO_EXTRACT:\n            continue\n        \n        class_name = TRIAL_TYPE_MAP[trial_type]\n        start_vol = int(onset / TR)\n        end_vol = int((onset + duration) / TR)\n        \n        for vol_idx in range(start_vol, min(end_vol, n_volumes)):\n            volume_3d = data_4d[:, :, :, vol_idx]\n            processed = preprocess_volume(volume_3d)\n            \n            out_filename = f\"{subject_id}_run-{run_id}_vol-{vol_idx:03d}.nii.gz\"\n            out_path = OUTPUT_DIR / class_name / out_filename\n            \n            nib.save(nib.Nifti1Image(processed, np.eye(4)), str(out_path))\n            counts[class_name] = counts.get(class_name, 0) + 1\n    \n    return counts\n\n# ==================== CELL 4: Process this batch ====================\nprint(f\"\\n{'='*60}\")\nprint(f\"PROCESSING BATCH {BATCH_NUMBER}\")\nprint(f\"{'='*60}\")\n\nresults = []\ntotal_counts = {}\n\nfor subject_id in tqdm(batch_subjects, desc=f\"Batch {BATCH_NUMBER}\"):\n    subject_files = list_subject_files(subject_id)\n    subject_counts = {}\n    \n    for denoised_key in subject_files:\n        events_key = get_events_key(denoised_key)\n        run_id = denoised_key.split('_run-')[1].split('_')[0]\n        \n        local_denoised = RAW_DIR / f\"{subject_id}_run-{run_id}_denoised.nii.gz\"\n        local_events = RAW_DIR / f\"{subject_id}_run-{run_id}_events.tsv\"\n        \n        if not download_file(denoised_key, local_denoised):\n            continue\n        if not download_file(events_key, local_events):\n            local_denoised.unlink() if local_denoised.exists() else None\n            continue\n        \n        try:\n            counts = extract_volumes(str(local_denoised), str(local_events), subject_id, run_id)\n            for cls, cnt in counts.items():\n                subject_counts[cls] = subject_counts.get(cls, 0) + cnt\n                total_counts[cls] = total_counts.get(cls, 0) + cnt\n        except Exception as e:\n            print(f\"  {subject_id} run-{run_id}: ERROR - {e}\")\n        \n        # Clean up immediately\n        local_denoised.unlink() if local_denoised.exists() else None\n        local_events.unlink() if local_events.exists() else None\n    \n    results.append((subject_id, sum(subject_counts.values())))\n\n# ==================== CELL 5: Summary ====================\nprint(f\"\\n{'='*60}\")\nprint(f\"BATCH {BATCH_NUMBER} COMPLETE!\")\nprint(f\"{'='*60}\")\n\ntotal_volumes = sum(r[1] for r in results)\nprint(f\"\\nSubjects processed: {len(results)}\")\nprint(f\"Total volumes extracted: {total_volumes:,}\")\n\nprint(\"\\nClass distribution:\")\nfor class_name in sorted(total_counts.keys()):\n    print(f\"  {class_name}: {total_counts[class_name]:,}\")\n\nprint(f\"\\nOutput saved to: {OUTPUT_DIR}\")\n\n# Show disk usage\nimport subprocess\nresult = subprocess.run(['du', '-sh', str(OUTPUT_DIR)], capture_output=True, text=True)\nprint(f\"Disk usage: {result.stdout.strip()}\")\n\n# ==================== CELL 6: Instructions ====================\nprint(f\"\\n{'='*60}\")\nprint(\"NEXT STEPS:\")\nprint(f\"{'='*60}\")\nprint(f\"\"\"\n1. Download this batch's output from:\n   {OUTPUT_DIR}\n\n2. Upload to OneDrive folder:\n   /thesis_data/batch_{BATCH_NUMBER:02d}/\n\n3. For next batch, create new notebook with:\n   BATCH_NUMBER = {BATCH_NUMBER + 1}\n\n4. Repeat until all batches complete.\n\nBATCH STATUS:\n\"\"\")\n\ntotal_batches = (len(all_subjects) + SUBJECTS_PER_BATCH - 1) // SUBJECTS_PER_BATCH\nfor b in range(1, total_batches + 1):\n    status = \"✓ DONE\" if b < BATCH_NUMBER else (\"→ CURRENT\" if b == BATCH_NUMBER else \"  pending\")\n    start = (b - 1) * SUBJECTS_PER_BATCH\n    end = min(start + SUBJECTS_PER_BATCH, len(all_subjects))\n    subjects_range = f\"sub-{all_subjects[start].split('-')[1]} to sub-{all_subjects[end-1].split('-')[1]}\" if end > start else \"none\"\n    print(f\"  Batch {b}: {subjects_range} {status}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}